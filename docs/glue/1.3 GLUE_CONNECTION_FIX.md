# AWS Glue 网络连接问题修复指南

## 问题描述

**customer-data-cleansing** job 失败，错误信息：
```
java.net.ConnectException: Connection refused
Failed to connect to /10.24.204.229:38621
```

这表示 Spark executor 无法与 driver 通信，导致任务失败。

## 根本原因

该错误通常是由以下任何一个原因引起的：

1. **VPC 配置缺失** - Glue job 在私有 VPC 中运行，但没有正确配置
2. **安全组规则不足** - 没有允许 executor 和 driver 之间的通信
3. **网络超时设置过短** - Spark 网络超时时间不足
4. **网络不稳定** - 内部网络延迟或中断

## 实施的修复

### 1. Terraform 配置改进

#### a. 添加 VPC 变量 (`infra/modules/glue/variables.tf`)
```hcl
variable "vpc_id" {
  description = "VPC ID for Glue job security group"
  type        = string
  default     = ""
}

variable "subnet_ids" {
  description = "Subnet IDs for Glue job"
  type        = list(string)
  default     = []
}

variable "security_group_ids" {
  description = "Security group IDs for Glue job"
  type        = list(string)
  default     = []
}
```

#### b. 创建安全组 (`infra/modules/glue/security.tf`)
自动为 Glue job 创建安全组，允许：
- 节点之间的自我通信（所有 TCP 端口）
- Spark 心跳端口（7077-7078）
- Spark RPC 端口（38621）

#### c. 更新 Job 配置 (`infra/modules/glue/jobs.tf`)
- 添加动态 VPC 配置块
- 当配置了 subnet_ids 时启用 FLEX 执行模式

#### d. 更新根配置 (`infra/main.tf`)
添加 VPC 参数示例（当前为空，可根据需要填入）。

### 2. Spark 配置增强

在两个 Glue 脚本中增强了 Spark 配置：
- `glue_scripts/1_data_cleansing.py`
- `glue_scripts/2_feature_engineering.py`

#### 网络连接优化
```python
spark_conf.set("spark.network.timeout", "600s")           # 增加到600秒
spark_conf.set("spark.executor.heartbeatInterval", "120s") # 增加到120秒
spark_conf.set("spark.rpc.numRetries", "10")              # 10次重试
spark_conf.set("spark.shuffle.io.maxRetries", "5")        # Shuffle重试
```

#### 内存和资源配置
```python
spark_conf.set("spark.driver.memory", "4g")
spark_conf.set("spark.executor.memory", "4g")
spark_conf.set("spark.executor.cores", "4")
spark_conf.set("spark.default.parallelism", "8")
```

#### 容错性增强
```python
spark_conf.set("spark.executor.maxFailures", "5")
spark_conf.set("spark.task.maxFailures", "5")
spark_conf.set("spark.speculation", "true")
```

## 如何使用修复

### 场景 1：Glue Job 在公有 VPC 或无 VPC（不推荐）
当前配置已支持，无需额外操作。修复后的 Spark 配置会自动应用。

### 场景 2：Glue Job 在私有 VPC（推荐）
如果你的 Glue job 在私有 VPC 中运行，需要：

1. **获取你的 VPC 信息**
   ```bash
   # 获取 VPC ID
   aws ec2 describe-vpcs --query 'Vpcs[0].VpcId'

   # 获取子网 IDs
   aws ec2 describe-subnets --query 'Subnets[*].[SubnetId,VpcId]' \
     --filters "Name=vpc-id,Values=vpc-xxxxx"
   ```

2. **更新 `infra/main.tf`**
   ```hcl
   module "glue_pipeline" {
     # ... 其他配置 ...

     # 填入你的实际 VPC 信息
     vpc_id          = "vpc-xxxxxxxx"
     subnet_ids      = ["subnet-xxxxxxxx", "subnet-yyyyyyyy"]
     # security_group_ids 会自动创建，或填入现有的
   }
   ```

3. **应用 Terraform 改动**
   ```bash
   cd infra
   terraform plan
   terraform apply
   ```

4. **重新运行 Glue Job**
   ```bash
   aws glue start-job-run --job-name customer-data-cleansing
   ```

## 监控和验证

### 检查 Job 日志
```bash
aws logs tail /aws-glue/jobs/customer-data-cleansing --follow
```

### CloudWatch 监控
- 检查 `CustomerDataPipeline` 命名空间的指标
- 监控 `CustomerBaseRows` 和 `CustomerBehaviorRows` 指标

## 故障排除

### 如果仍然出现连接错误

1. **检查安全组规则**
   ```bash
   aws ec2 describe-security-groups --group-ids sg-xxxxxxxx
   ```

2. **验证网络连通性**
   - 确保子网之间可以相互通信
   - 检查 NAT 网关（如果使用私有子网）
   - 验证路由表配置

3. **增加超时和重试**
   如果网络延迟较高，可进一步增加超时值：
   ```python
   spark_conf.set("spark.network.timeout", "900s")
   spark_conf.set("spark.executor.heartbeatInterval", "180s")
   spark_conf.set("spark.rpc.numRetries", "15")
   ```

4. **检查 Glue Job 配置**
   ```bash
   aws glue get-job --name customer-data-cleansing
   ```

## 性能考虑

- 增加网络超时可能会略微降低故障检测速度，但提高稳定性
- FLEX 执行模式在私有 VPC 中表现更好
- 推测执行有助于应对不均匀的任务执行时间

## 相关文件变更

| 文件 | 变更内容 |
|------|---------|
| `infra/modules/glue/variables.tf` | 添加 VPC 配置变量 |
| `infra/modules/glue/security.tf` | 新建，创建安全组 |
| `infra/modules/glue/jobs.tf` | 添加 VPC 配置块 |
| `infra/main.tf` | 添加 VPC 参数传入 |
| `glue_scripts/1_data_cleansing.py` | 增强 Spark 网络配置 |
| `glue_scripts/2_feature_engineering.py` | 增强 Spark 网络配置 |

## 下一步

1. 根据你的环境填入 VPC 配置
2. 运行 `terraform plan` 验证改动
3. 运行 `terraform apply` 部署
4. 重新运行 customer-data-cleansing job
5. 监控 CloudWatch 日志确认成功

如有问题，检查 CloudWatch 日志中的详细错误信息。
