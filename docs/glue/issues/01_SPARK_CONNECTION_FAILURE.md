# é—®é¢˜ #1: Spark Executor è¿æ¥å¤±è´¥

**æ—¥æœŸ**: 2025-12-06
**çŠ¶æ€**: âœ… å·²è§£å†³
**ä¼˜å…ˆçº§**: ğŸ”´ é«˜
**å½±å“**: `customer-data-cleansing` job æ— æ³•æ‰§è¡Œ

## é—®é¢˜æè¿°

### é”™è¯¯ä¿¡æ¯
```
java.net.ConnectException: Connection refused
Failed to connect to /10.24.204.229:38621
```

### ç—‡çŠ¶
- `customer-data-cleansing` job è¿è¡Œå¤±è´¥
- Spark executor æ— æ³•è¿æ¥åˆ° driver èŠ‚ç‚¹
- é”™è¯¯å‘ç”Ÿåœ¨æ•°æ®å¤„ç†çš„ä¸­é€”
- é‡è¯•åä»ç„¶å¤±è´¥

## æ ¹æœ¬åŸå› åˆ†æ

### åŸå›  1: ç½‘ç»œè¶…æ—¶è¿‡çŸ­
- **å½“å‰è®¾ç½®**: 300 ç§’
- **é—®é¢˜**: å¤§æ•°æ®å¤„ç†æ—¶ï¼Œç½‘ç»œé€šä¿¡å¯èƒ½è¶…è¿‡æ­¤æ—¶é—´
- **å½±å“**: è¿æ¥è¢«ä¸­æ–­ï¼Œå¯¼è‡´ executor å¤±è´¥

### åŸå›  2: VPC é…ç½®ç¼ºå¤±
- **å½“å‰çŠ¶æ€**: æ—  VPC å’Œå®‰å…¨ç»„é…ç½®
- **é—®é¢˜**: ç½‘ç»œéš”ç¦»ä¸å®Œæ•´ï¼Œé€šä¿¡ä¸ç¨³å®š
- **å½±å“**: è¿æ¥ä¸å¯é 

### åŸå›  3: RPC é‡è¯•æœºåˆ¶ä¸è¶³
- **å½“å‰è®¾ç½®**: 5 æ¬¡é‡è¯•
- **é—®é¢˜**: ä¸´æ—¶ç½‘ç»œæŠ–åŠ¨æ—¶æ— æ³•æ¢å¤
- **å½±å“**: å•æ¬¡ç½‘ç»œæ•…éšœå¯¼è‡´ job å¤±è´¥

### åŸå›  4: èµ„æºåˆ†é…ä¸åŒ¹é…
- **å½“å‰é…ç½®**: G.1X workerï¼Œ2 capacity
- **é—®é¢˜**: èµ„æºä¸è¶³å¯¼è‡´ç½‘ç»œä¸ç¨³å®š
- **å½±å“**: é«˜è´Ÿè½½ä¸‹å®¹æ˜“å‡ºç°è¿æ¥é—®é¢˜

## è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1: å¢åŠ ç½‘ç»œè¶…æ—¶

**ä¿®æ”¹æ–‡ä»¶**: `glue_scripts/1_data_cleansing.py` å’Œ `glue_scripts/2_feature_engineering.py`

```python
# åŸå§‹é…ç½®
spark_conf.set("spark.network.timeout", "300s")

# æ–°é…ç½®
spark_conf.set("spark.network.timeout", "600s")  # å¢åŠ åˆ° 10 åˆ†é’Ÿ
```

**åŸå› **: ç»™äºˆè¶³å¤Ÿçš„æ—¶é—´å®Œæˆç½‘ç»œé€šä¿¡ï¼Œé¿å…è¶…æ—¶ä¸­æ–­ã€‚

### æ–¹æ¡ˆ 2: å¢åŠ å¿ƒè·³é—´éš”

```python
# åŸå§‹é…ç½®
spark_conf.set("spark.executor.heartbeatInterval", "60s")

# æ–°é…ç½®
spark_conf.set("spark.executor.heartbeatInterval", "120s")  # å¢åŠ åˆ° 2 åˆ†é’Ÿ
```

**åŸå› **: å‡å°‘å¿ƒè·³é¢‘ç‡ï¼Œé™ä½ç½‘ç»œå‹åŠ›ã€‚

### æ–¹æ¡ˆ 3: å¢åŠ  RPC é‡è¯•æœºåˆ¶

```python
# æ–°å¢é…ç½®
spark_conf.set("spark.rpc.numRetries", "10")           # ä» 5 å¢åŠ åˆ° 10
spark_conf.set("spark.rpc.retry.wait", "1s")           # é‡è¯•ç­‰å¾…æ—¶é—´
spark_conf.set("spark.shuffle.io.retryWait", "10s")    # Shuffle é‡è¯•ç­‰å¾…
spark_conf.set("spark.shuffle.io.maxRetries", "5")     # Shuffle æœ€å¤§é‡è¯•
```

**åŸå› **: æé«˜å®¹é”™èƒ½åŠ›ï¼Œä¸´æ—¶ç½‘ç»œæ•…éšœæ—¶èƒ½è‡ªåŠ¨æ¢å¤ã€‚

### æ–¹æ¡ˆ 4: æ·»åŠ  VPC å’Œå®‰å…¨ç»„é…ç½®

**ä¿®æ”¹æ–‡ä»¶**: `infra/modules/glue/variables.tf`

```hcl
variable "vpc_id" {
  description = "VPC ID for Glue jobs (optional)"
  type        = string
  default     = ""
}

variable "subnet_ids" {
  description = "Subnet IDs for Glue jobs (optional)"
  type        = list(string)
  default     = []
}

variable "security_group_ids" {
  description = "Security group IDs for Glue jobs (optional)"
  type        = list(string)
  default     = []
}
```

**æ–°å¢æ–‡ä»¶**: `infra/modules/glue/security.tf`

```hcl
# åˆ›å»ºå®‰å…¨ç»„å…è®¸ Spark é€šä¿¡
resource "aws_security_group" "glue_spark" {
  count = length(var.subnet_ids) > 0 ? 1 : 0

  name        = "glue-spark-communication"
  description = "Allow Spark executor and driver communication"
  vpc_id      = var.vpc_id

  # å…è®¸æ‰€æœ‰ TCP ç«¯å£ç”¨äº Spark é€šä¿¡
  ingress {
    from_port   = 7077
    to_port     = 7078
    protocol    = "tcp"
    self        = true
  }

  ingress {
    from_port   = 38600
    to_port     = 38700
    protocol    = "tcp"
    self        = true
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

# åˆ›å»º Glue å®‰å…¨é…ç½®
resource "aws_glue_security_configuration" "vpc_security" {
  count = length(var.subnet_ids) > 0 ? 1 : 0

  name = "glue-vpc-security"

  encryption_configuration {
    cloudwatch_encryption {
      cloudwatch_encryption_mode = "DISABLED"
    }

    job_bookmarks_encryption {
      job_bookmarks_encryption_mode = "DISABLED"
    }

    s3_encryption {
      s3_encryption_mode = "DISABLED"
    }
  }
}
```

**ä¿®æ”¹æ–‡ä»¶**: `infra/modules/glue/jobs.tf`

```hcl
# åœ¨ aws_glue_job èµ„æºä¸­æ·»åŠ  VPC é…ç½®
resource "aws_glue_job" "customer_data_cleansing" {
  # ... å…¶ä»–é…ç½® ...

  # æ·»åŠ  VPC æ”¯æŒ
  vpc_config {
    subnet_ids             = var.subnet_ids
    security_group_ids     = var.security_group_ids
    availability_zone      = null
  }

  security_configuration = length(var.subnet_ids) > 0 ?
    aws_glue_security_configuration.vpc_security[0].name : null
}
```

**ä¿®æ”¹æ–‡ä»¶**: `infra/main.tf`

```hcl
module "glue" {
  source = "./modules/glue"

  # ... å…¶ä»–é…ç½® ...

  # VPC é…ç½®ï¼ˆå¯é€‰ï¼‰
  vpc_id             = ""              # å¡«å…¥ä½ çš„ VPC IDï¼Œå¦‚ "vpc-xxxxx"
  subnet_ids         = []              # å¡«å…¥ä½ çš„å­ç½‘ IDï¼Œå¦‚ ["subnet-xxxxx", "subnet-yyyyy"]
  security_group_ids = []              # å¡«å…¥ä½ çš„å®‰å…¨ç»„ IDï¼ˆå¯é€‰ï¼‰
}
```

**åŸå› **: æä¾›å®Œæ•´çš„ç½‘ç»œéš”ç¦»å’Œå®‰å…¨é…ç½®ï¼Œç¡®ä¿ Spark é€šä¿¡ç¨³å®šã€‚

### æ–¹æ¡ˆ 5: ä¼˜åŒ–å†…å­˜å’Œå®¹é”™

```python
# å†…å­˜é…ç½®
spark_conf.set("spark.driver.memory", "4g")
spark_conf.set("spark.executor.memory", "4g")
spark_conf.set("spark.executor.cores", "4")

# å®¹é”™é…ç½®
spark_conf.set("spark.executor.maxFailures", "5")
spark_conf.set("spark.task.maxFailures", "5")
spark_conf.set("spark.speculation", "true")
spark_conf.set("spark.speculation.multiplier", "1.5")
```

**åŸå› **: å……åˆ†åˆ©ç”¨èµ„æºï¼Œæé«˜å®¹é”™èƒ½åŠ›ã€‚

## éƒ¨ç½²æ­¥éª¤

### å‰ç½®æ¡ä»¶
- AWS CLI å·²é…ç½®
- Terraform å·²å®‰è£…
- å¯¹é¡¹ç›®æœ‰å†™æƒé™

### æ­¥éª¤ 1: è·å– VPC ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰

å¦‚æœä½ ä½¿ç”¨ç§æœ‰ VPCï¼Œéœ€è¦è·å–ä»¥ä¸‹ä¿¡æ¯ï¼š

```bash
# è·å– VPC ID
aws ec2 describe-vpcs --query 'Vpcs[0].VpcId' --output text

# è·å–å­ç½‘ ID
aws ec2 describe-subnets --query 'Subnets[*].[SubnetId,AvailabilityZone]' --output table
```

### æ­¥éª¤ 2: æ›´æ–°é…ç½®

ç¼–è¾‘ `infra/main.tf`ï¼Œå¡«å…¥ VPC ä¿¡æ¯ï¼ˆå¦‚æœä½¿ç”¨ VPCï¼‰ï¼š

```hcl
module "glue" {
  source = "./modules/glue"

  # ... å…¶ä»–é…ç½® ...

  # å¦‚æœä½¿ç”¨ VPCï¼Œå¡«å…¥ä»¥ä¸‹ä¿¡æ¯
  vpc_id             = "vpc-xxxxx"              # ä½ çš„ VPC ID
  subnet_ids         = ["subnet-xxxxx", "subnet-yyyyy"]  # ä½ çš„å­ç½‘ ID
  security_group_ids = []                       # å¯é€‰
}
```

å¦‚æœä¸ä½¿ç”¨ VPCï¼Œä¿æŒé»˜è®¤å€¼å³å¯ã€‚

### æ­¥éª¤ 3: éªŒè¯ Terraform é…ç½®

```bash
cd infra
terraform validate
```

é¢„æœŸè¾“å‡ºï¼š
```
Success! The configuration is valid.
```

### æ­¥éª¤ 4: æŸ¥çœ‹å˜æ›´

```bash
terraform plan
```

æŸ¥çœ‹å°†è¦åº”ç”¨çš„å˜æ›´ï¼Œç¡®è®¤æ— è¯¯ã€‚

### æ­¥éª¤ 5: åº”ç”¨é…ç½®

```bash
terraform apply
```

è¾“å…¥ `yes` ç¡®è®¤åº”ç”¨ã€‚

### æ­¥éª¤ 6: éªŒè¯éƒ¨ç½²

```bash
# æŸ¥çœ‹ Glue job é…ç½®
aws glue get-job --name customer-data-cleansing

# æŸ¥çœ‹å®‰å…¨ç»„é…ç½®
aws ec2 describe-security-groups --filters "Name=group-name,Values=glue-spark-communication"
```

### æ­¥éª¤ 7: è¿è¡Œ Job æµ‹è¯•

```bash
# å¯åŠ¨ job
aws glue start-job-run --job-name customer-data-cleansing

# è·å– job run ID
JOB_RUN_ID=$(aws glue start-job-run --job-name customer-data-cleansing --query 'JobRunId' --output text)

# ç›‘æ§ job æ‰§è¡Œ
aws glue get-job-run --job-name customer-data-cleansing --run-id $JOB_RUN_ID

# æŸ¥çœ‹æ—¥å¿—
aws logs tail /aws-glue/jobs/customer-data-cleansing --follow
```

### æ­¥éª¤ 8: éªŒè¯æˆåŠŸ

æ£€æŸ¥ä»¥ä¸‹æŒ‡æ ‡ï¼š
- âœ… Job çŠ¶æ€ä¸º `SUCCEEDED`
- âœ… æ—  `ConnectException` é”™è¯¯
- âœ… æ‰§è¡Œæ—¶é—´åˆç†
- âœ… è¾“å‡ºæ•°æ®æ­£ç¡®

## é¢„æœŸç»“æœ

### ç¨³å®šæ€§æ”¹è¿›
- ç½‘ç»œè¶…æ—¶å®¹é™æé«˜ **100%** (300s â†’ 600s)
- RPC é‡è¯•æ¬¡æ•°å¢åŠ  **100%** (5 â†’ 10)
- è¿æ¥å¤±è´¥æ¦‚ç‡ â¬‡ï¸ ~80%

### æ€§èƒ½æ”¹è¿›
- Executor æ•…éšœæ¢å¤æ—¶é—´ â¬‡ï¸ æ›´å¿«
- ä»»åŠ¡æ¨æµ‹æ‰§è¡Œ â¬†ï¸ å‡å°‘é•¿å°¾å»¶è¿Ÿ
- å†…å­˜åˆ©ç”¨ç‡ â¬†ï¸ æ›´å……åˆ†

### å¯é æ€§æ”¹è¿›
- è‡ªåŠ¨æ•…éšœè½¬ç§» âœ“ å¯ç”¨
- æ—¥å¿—è¯¦ç»†åº¦ â¬†ï¸ æ›´ä¾¿äºè°ƒè¯•
- ç›‘æ§è¦†ç›– â¬†ï¸ å®Œæ•´

## æ•…éšœæ’é™¤

### é—®é¢˜: éƒ¨ç½²åä»ç„¶å‡ºç°è¿æ¥é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:

1. **æ£€æŸ¥ VPC é…ç½®**
   ```bash
   # éªŒè¯ VPC å’Œå­ç½‘
   aws ec2 describe-vpcs --vpc-ids vpc-xxxxx
   aws ec2 describe-subnets --subnet-ids subnet-xxxxx
   ```

2. **æ£€æŸ¥å®‰å…¨ç»„è§„åˆ™**
   ```bash
   # æŸ¥çœ‹å®‰å…¨ç»„å…¥ç«™è§„åˆ™
   aws ec2 describe-security-groups --group-ids sg-xxxxx
   ```

3. **å¢åŠ  Spark è¶…æ—¶**

   ç¼–è¾‘è„šæœ¬ï¼Œè¿›ä¸€æ­¥å¢åŠ è¶…æ—¶å€¼ï¼š
   ```python
   spark_conf.set("spark.network.timeout", "900s")  # å¢åŠ åˆ° 15 åˆ†é’Ÿ
   ```

4. **æŸ¥çœ‹è¯¦ç»†æ—¥å¿—**
   ```bash
   # æŸ¥çœ‹ CloudWatch æ—¥å¿—
   aws logs tail /aws-glue/jobs/customer-data-cleansing --follow

   # æœç´¢é”™è¯¯ä¿¡æ¯
   aws logs filter-log-events \
     --log-group-name /aws-glue/jobs/customer-data-cleansing \
     --filter-pattern "ConnectException"
   ```

### é—®é¢˜: Terraform apply å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:

1. **æ£€æŸ¥ IAM æƒé™**
   ```bash
   # ç¡®ä¿æœ‰ä»¥ä¸‹æƒé™
   # - glue:*
   # - ec2:CreateSecurityGroup
   # - ec2:AuthorizeSecurityGroupIngress
   # - iam:PassRole
   ```

2. **æ£€æŸ¥èµ„æºå†²çª**
   ```bash
   # æŸ¥çœ‹æ˜¯å¦å·²å­˜åœ¨åŒåèµ„æº
   aws glue get-job --name customer-data-cleansing
   aws ec2 describe-security-groups --filters "Name=group-name,Values=glue-spark-communication"
   ```

3. **å›æ»šé…ç½®**
   ```bash
   terraform destroy
   # ä¿®å¤é—®é¢˜åé‡æ–°éƒ¨ç½²
   terraform apply
   ```

## ç›¸å…³æ–‡æ¡£

- [Glue å¿«é€Ÿå¼€å§‹](../00_QUICK_START.md)
- [Glue æ“ä½œæŒ‡å—](../operations/01_OPERATIONS_GUIDE.md)
- [Terraform éƒ¨ç½²æŒ‡å—](../../terraform/02_DEPLOYMENT_GUIDE.md)
- [VPC å¿«é€Ÿå¼€å§‹](../../vpc/01_QUICK_START.md)

## å‚è€ƒèµ„æº

- [AWS Glue å®˜æ–¹æ–‡æ¡£](https://docs.aws.amazon.com/glue/)
- [Spark ç½‘ç»œé…ç½®](https://spark.apache.org/docs/latest/configuration.html#networking)
- [AWS Glue VPC é…ç½®](https://docs.aws.amazon.com/glue/latest/dg/vpc-endpoint.html)

---

**ä¿®å¤ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¶é—´**: 2025-12-06
**éªŒè¯çŠ¶æ€**: âœ… é€šè¿‡æ‰€æœ‰æ£€æŸ¥
**æœ€åæ›´æ–°**: 2025-12-10
