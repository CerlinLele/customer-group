# AWS Glue Pipeline å¿«é€Ÿæ‰§è¡Œå‚è€ƒ

## å¿«é€Ÿå¼€å§‹ï¼ˆæ¨èï¼‰

### 1ï¸âƒ£ éƒ¨ç½²åŸºç¡€è®¾æ–½

```bash
cd infra
terraform init
terraform apply
```

### 2ï¸âƒ£ è¿è¡ŒåŸå§‹æ•°æ®çˆ¬è™«

```bash
# è¿è¡Œçˆ¬è™«æ¥æ‰«æ CSV æ–‡ä»¶
aws glue start-crawler --name "case-dev-raw-customer-base-crawler"
aws glue start-crawler --name "case-dev-raw-customer-behavior-crawler"

# ç­‰å¾…çˆ¬è™«å®Œæˆï¼ˆç›‘æ§çŠ¶æ€ï¼‰
watch -n 5 'aws glue get-crawler --name case-dev-raw-customer-base-crawler | grep State'
```

**é¢„æœŸæ—¶é—´**: 1-2 åˆ†é’Ÿ

### 3ï¸âƒ£ éªŒè¯åŸå§‹è¡¨

```bash
# åˆ—å‡ºåŸå§‹è¡¨
aws glue get-tables --database-name customer_raw_db

# æŸ¥è¯¢éªŒè¯
aws athena start-query-execution \
  --query-string "SELECT COUNT(*) FROM customer_raw_db.raw_customer_base" \
  --query-execution-context Database=customer_raw_db \
  --result-configuration OutputLocation=s3://your-bucket/athena-results/
```

### 4ï¸âƒ£ è¿è¡Œæ•°æ®æ¸…æ´—ä½œä¸š

```bash
# å¯åŠ¨ä½œä¸š
aws glue start-job-run --job-name customer-data-cleansing

# è·å–ä½œä¸š ID
JOB_RUN_ID=$(aws glue start-job-run --job-name customer-data-cleansing --query 'JobRunId' --output text)
echo "Job Run ID: $JOB_RUN_ID"

# ç›‘æ§ä½œä¸šè¿›åº¦
aws glue get-job-run --job-name customer-data-cleansing --run-id $JOB_RUN_ID

# æŸ¥çœ‹å®æ—¶æ—¥å¿—
aws logs tail "/aws-glue/jobs/customer-data-cleansing" --follow
```

**é¢„æœŸæ—¶é—´**: 5-15 åˆ†é’Ÿ

### 5ï¸âƒ£ è¿è¡Œæ¸…æ´—åæ•°æ®çˆ¬è™«

```bash
# ç­‰å¾…ä¸Šä¸€æ­¥ä½œä¸šå®Œæˆåï¼Œè¿è¡Œçˆ¬è™«
aws glue start-crawler --name "case-dev-cleaned-customer-base-crawler"
aws glue start-crawler --name "case-dev-cleaned-customer-behavior-crawler"

# ç›‘æ§çŠ¶æ€
watch -n 5 'aws glue get-crawler --name case-dev-cleaned-customer-base-crawler | grep State'
```

**é¢„æœŸæ—¶é—´**: 1-2 åˆ†é’Ÿ

### 6ï¸âƒ£ éªŒè¯æ¸…æ´—è¡¨

```bash
# åˆ—å‡ºæ¸…æ´—è¡¨
aws glue get-tables --database-name customer_cleaned_db

# æŸ¥è¯¢éªŒè¯
SELECT * FROM customer_cleaned_db.cleaned_customer_base LIMIT 5;
SELECT COUNT(*) FROM customer_cleaned_db.cleaned_customer_behavior;
```

### 7ï¸âƒ£ è¿è¡Œç‰¹å¾å·¥ç¨‹ä½œä¸š

```bash
# å¯åŠ¨ä½œä¸š
aws glue start-job-run --job-name customer-feature-engineering

# ç›‘æ§å’ŒæŸ¥çœ‹æ—¥å¿—
JOB_RUN_ID=$(aws glue start-job-run --job-name customer-feature-engineering --query 'JobRunId' --output text)
aws logs tail "/aws-glue/jobs/customer-feature-engineering" --follow
```

**é¢„æœŸæ—¶é—´**: 10-20 åˆ†é’Ÿ

---

## å®Œæ•´å‘½ä»¤è„šæœ¬

### ä¸€é”®éƒ¨ç½²ï¼ˆå¦‚æœæ‰€æœ‰å…ˆå†³æ¡ä»¶å·²æ»¡è¶³ï¼‰

```bash
#!/bin/bash
set -e

echo "=========================================="
echo "AWS Glue Pipeline - å®Œæ•´è‡ªåŠ¨åŒ–æ‰§è¡Œ"
echo "=========================================="

# é…ç½®å˜é‡
PROJECT_NAME="case"
ENVIRONMENT="dev"
REGION="us-east-1"

# Step 1: éƒ¨ç½² Terraform
echo "Step 1: éƒ¨ç½² Terraform åŸºç¡€è®¾æ–½..."
cd infra
terraform apply -auto-approve
cd ..

# Step 2: è¿è¡ŒåŸå§‹æ•°æ®çˆ¬è™«
echo "Step 2: è¿è¡ŒåŸå§‹æ•°æ®çˆ¬è™«..."
aws glue start-crawler --name "$PROJECT_NAME-$ENVIRONMENT-raw-customer-base-crawler" --region $REGION
aws glue start-crawler --name "$PROJECT_NAME-$ENVIRONMENT-raw-customer-behavior-crawler" --region $REGION

# ç­‰å¾…çˆ¬è™«å®Œæˆ
echo "ç­‰å¾…çˆ¬è™«å®Œæˆ..."
while true; do
  CRAWLER_STATE=$(aws glue get-crawler --name "$PROJECT_NAME-$ENVIRONMENT-raw-customer-base-crawler" --region $REGION --query 'Crawler.State' --output text)
  if [ "$CRAWLER_STATE" = "READY" ]; then
    echo "çˆ¬è™«å·²å®Œæˆ"
    break
  fi
  echo "çˆ¬è™«æ­£åœ¨è¿è¡Œ... (çŠ¶æ€: $CRAWLER_STATE)"
  sleep 10
done

# Step 3: è¿è¡Œæ•°æ®æ¸…æ´—ä½œä¸š
echo "Step 3: è¿è¡Œæ•°æ®æ¸…æ´—ä½œä¸š..."
JOB_RUN_ID=$(aws glue start-job-run --job-name customer-data-cleansing --region $REGION --query 'JobRunId' --output text)
echo "ä½œä¸šè¿è¡Œ ID: $JOB_RUN_ID"

# ç­‰å¾…ä½œä¸šå®Œæˆ
echo "ç­‰å¾…æ•°æ®æ¸…æ´—ä½œä¸šå®Œæˆ..."
while true; do
  JOB_STATE=$(aws glue get-job-run --job-name customer-data-cleansing --run-id $JOB_RUN_ID --region $REGION --query 'JobRun.JobRunState' --output text)
  if [ "$JOB_STATE" = "SUCCEEDED" ]; then
    echo "æ•°æ®æ¸…æ´—ä½œä¸šå·²å®Œæˆ"
    break
  elif [ "$JOB_STATE" = "FAILED" ]; then
    echo "æ•°æ®æ¸…æ´—ä½œä¸šå¤±è´¥ï¼"
    exit 1
  fi
  echo "ä½œä¸šçŠ¶æ€: $JOB_STATEï¼Œç­‰å¾…ä¸­..."
  sleep 30
done

# Step 4: è¿è¡Œæ¸…æ´—åæ•°æ®çˆ¬è™«
echo "Step 4: è¿è¡Œæ¸…æ´—åæ•°æ®çˆ¬è™«..."
aws glue start-crawler --name "$PROJECT_NAME-$ENVIRONMENT-cleaned-customer-base-crawler" --region $REGION
aws glue start-crawler --name "$PROJECT_NAME-$ENVIRONMENT-cleaned-customer-behavior-crawler" --region $REGION

# ç­‰å¾…çˆ¬è™«å®Œæˆ
echo "ç­‰å¾…æ¸…æ´—çˆ¬è™«å®Œæˆ..."
while true; do
  CRAWLER_STATE=$(aws glue get-crawler --name "$PROJECT_NAME-$ENVIRONMENT-cleaned-customer-base-crawler" --region $REGION --query 'Crawler.State' --output text)
  if [ "$CRAWLER_STATE" = "READY" ]; then
    echo "æ¸…æ´—çˆ¬è™«å·²å®Œæˆ"
    break
  fi
  echo "çˆ¬è™«æ­£åœ¨è¿è¡Œ... (çŠ¶æ€: $CRAWLER_STATE)"
  sleep 10
done

# Step 5: è¿è¡Œç‰¹å¾å·¥ç¨‹ä½œä¸š
echo "Step 5: è¿è¡Œç‰¹å¾å·¥ç¨‹ä½œä¸š..."
JOB_RUN_ID=$(aws glue start-job-run --job-name customer-feature-engineering --region $REGION --query 'JobRunId' --output text)
echo "ä½œä¸šè¿è¡Œ ID: $JOB_RUN_ID"

echo ""
echo "=========================================="
echo "âœ… æ‰€æœ‰ä»»åŠ¡å·²å¯åŠ¨ï¼"
echo "=========================================="
echo "åŸå§‹çˆ¬è™«: âœ“"
echo "æ•°æ®æ¸…æ´—ä½œä¸š: âœ“"
echo "æ¸…æ´—çˆ¬è™«: âœ“"
echo "ç‰¹å¾å·¥ç¨‹ä½œä¸š: è¿è¡Œä¸­..."
echo ""
echo "è¦ç›‘æ§ç‰¹å¾å·¥ç¨‹ä½œä¸šï¼Œè¿è¡Œï¼š"
echo "aws logs tail '/aws-glue/jobs/customer-feature-engineering' --follow"
```

---

## æ•…éšœæ’æŸ¥å¿«é€Ÿæ£€æŸ¥è¡¨

### âŒ çˆ¬è™«æ˜¾ç¤º"FAILED"

```bash
# æŸ¥çœ‹çˆ¬è™«æ—¥å¿—
aws logs tail /aws-glue/crawlers/PROJECT-ENV-crawler-name --follow

# éªŒè¯ S3 è·¯å¾„
aws s3 ls s3://your-bucket/raw/
aws s3 ls s3://your-bucket/cleaned/customer_base/

# æ£€æŸ¥ IAM æƒé™
aws iam get-role-policy --role-name PROJECT-ENV-GlueCustomerDataRole --policy-name GlueExecutionPolicy
```

### âŒ Glue Job å¤±è´¥ï¼šEntityNotFoundException

```bash
# æ£€æŸ¥æ˜¯å¦æœ‰ç›¸åº”çš„è¡¨
aws glue get-table --database-name customer_raw_db --name raw_customer_base

# å¦‚æœè¡¨ä¸å­˜åœ¨ï¼Œè¿è¡Œçˆ¬è™«
aws glue start-crawler --name case-dev-raw-customer-base-crawler

# ç­‰å¾…çˆ¬è™«å®Œæˆ
aws glue wait crawler-ready --name case-dev-raw-customer-base-crawler
```

### âŒ Glue Job æ˜¾ç¤º"FAILED"

```bash
# è·å–ä½œä¸šè¿è¡Œ ID
JOB_RUN_ID=$(aws glue get-job-runs --job-name customer-data-cleansing --max-items 1 --query 'JobRuns[0].Id' --output text)

# æŸ¥çœ‹é”™è¯¯æ—¥å¿—
aws logs tail "/aws-glue/jobs/customer-data-cleansing" --follow

# è·å–è¯¦ç»†çš„ä½œä¸šè¿è¡Œä¿¡æ¯
aws glue get-job-run --job-name customer-data-cleansing --run-id $JOB_RUN_ID
```

---

## ç›¸å…³èµ„æº

- ğŸ“– [å®Œæ•´æ‰§è¡ŒæŒ‡å—](GLUE_PIPELINE_EXECUTION_GUIDE.md)
- ğŸ“ [Terraform é…ç½®](infra/modules/glue/crawlers.tf)
- ğŸ”§ [Glue ä½œä¸šé…ç½®](glue_scripts/config/glue_jobs_config.json)
- ğŸ“Š [æ•°æ®æ¸…æ´—è„šæœ¬](glue_scripts/1_data_cleansing.py)
- ğŸ¯ [ç‰¹å¾å·¥ç¨‹è„šæœ¬](glue_scripts/2_feature_engineering.py)

---

## æˆæœ¬ä¼°ç®—

| ç»„ä»¶ | ä¼°è®¡æˆæœ¬ | è¯´æ˜ |
|------|--------|------|
| Glue Crawlers | $0.44/çˆ¬è™«å°æ—¶ | æ¯ä¸ªçˆ¬è™«é€šå¸¸è¿è¡Œ 1-2 åˆ†é’Ÿ |
| Glue Jobs (G.2X) | $0.44/DPUå°æ—¶ | æ¸…æ´—ä½œä¸š: 5-15 åˆ†é’Ÿ; ç‰¹å¾ä½œä¸š: 10-20 åˆ†é’Ÿ |
| S3 å­˜å‚¨ | $0.023/GB/æœˆ | å–å†³äºåŸå§‹å’Œå¤„ç†æ•°æ®å¤§å° |
| Athena æŸ¥è¯¢ | $5 per TB | æŒ‰æ‰«ææ•°æ®è®¡è´¹ |
| æ€»è®¡ (æ¯æœˆ) | ~$20-50 | åŸºäºæ¯å¤©è¿è¡Œä¸€æ¬¡çš„å‡è®¾ |

---

## æŠ€å·§å’Œæœ€ä½³å®è·µ

### ğŸ’¡ èŠ‚çœæˆæœ¬

1. **ä½¿ç”¨ On-Demand Crawlers**: æ‰‹åŠ¨è§¦å‘è€Œä¸æ˜¯è®¡åˆ’è§¦å‘
2. **ä¼˜åŒ– Job èµ„æº**: æ ¹æ®æ•°æ®å¤§å°è°ƒæ•´ Worker æ•°é‡
3. **ä½¿ç”¨ Job Bookmarks**: åªå¤„ç†æ–°å¢æ•°æ®

### âš¡ åŠ å¿«æ‰§è¡Œ

1. **å¹¶è¡Œè¿è¡Œçˆ¬è™«**: å¯ä»¥åŒæ—¶è¿è¡Œå¤šä¸ªçˆ¬è™«
2. **è°ƒæ•´ Worker é…ç½®**: å¢åŠ  Worker æ•°é‡åŠ é€Ÿå¤„ç†
3. **ä½¿ç”¨ Parquet æ ¼å¼**: æ¯” CSV å¿« 10 å€ä»¥ä¸Š

### ğŸ”’ å®‰å…¨æœ€ä½³å®è·µ

1. **ä½¿ç”¨ Glue Connection**: ä¸ºæ•°æ®åº“è¿æ¥åŠ å¯†
2. **å¯ç”¨ S3 åŠ å¯†**: ä½¿ç”¨ KMS åŠ å¯†æ•æ„Ÿæ•°æ®
3. **é™åˆ¶ IAM æƒé™**: éµå¾ªæœ€å°æƒé™åŸåˆ™
4. **å¯ç”¨ CloudTrail**: å®¡è®¡æ‰€æœ‰ API è°ƒç”¨

